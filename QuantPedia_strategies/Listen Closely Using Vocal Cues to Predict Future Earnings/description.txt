The paper focuses on the prediction of firm earnings and introduces the concept of using vocal cues from manager speech as an additional source of information for earnings prediction. The authors argue that vocal cues, which reflect emotional states and cognitive shifts, may provide valuable insights into managers’ evaluations of current and future firm performance. The study proposes two different model architectures for analyzing vocal cues: a Convolutional Neural Network (CNN) that processes spectrograms of audio and wav2vec 2.0, an unsupervised pre-trained model for speech recognition. The authors use a dataset of audio recordings of earnings conference calls to extract vocal cues and predict the direction of one-year-ahead earnings changes. The results show that vocal cues from manager speech have significant predictive power, outperforming traditional financial data and textual inputs. The vocal cue models also demonstrate their economic relevance by generating profitable trading strategies and improving the performance of financial analysts. The authors contribute to the literature on earnings prediction, voice analysis in a corporate context, and the practical implications of new technologies and data sources in the financial domain. However, they acknowledge limitations, such as the availability of audio data and potential concerns about data leakage in deep learning architectures. Overall, the findings suggest that vocal cues provide valuable information for predicting future earnings and have untapped potential in investment decision-making.

Fundamental reason
The fundamental reason for the functionality of the strategy proposed in this paper is that vocal cues from manager speech provide incremental information about future firm earnings. The paper argues that managers’ vocal cues, which reflect their genuine emotional states, can convey information about their internal evaluations of current and future firm performance. Unlike numerical financial data and well-thought-out disclosures, speech production and vocal cues are difficult to control and can inadvertently reveal information that managers do not intend to disclose. The strategy focuses on analyzing vocal cues from audio data using two different model architectures: Convolutional Neural Networks (CNN) and wav2vec 2.0. These models are designed to integrate big data analytics and account for the granularity and sequential nature of vocal cues. By analyzing audio recordings of earnings conference calls, where managers interact with financial analysts and investors, the study identifies predictive vocal cues that can be used to predict the direction of one-year-ahead earnings changes. The paper demonstrates that the proposed vocal cue models have significant out-of-sample predictive power and outperform existing earnings prediction models based on financial data and textual inputs. The models also show potential economic relevance by generating excess returns in trading strategies and improving the performance of financial analysts. Overall, the functionality of the strategy lies in leveraging vocal cues as a novel and theoretically grounded approach to predicting future earnings changes.

Simple trading strategy
1. Theoretical Background
The paper provides a theoretical background for the research on predicting future earnings and introduces the institutional setting of the study. The section begins by highlighting the importance of earnings for assessing a firm’s success, comparing it to peers, and determining firm value and share prices. Previous research has shown that predicting earnings is challenging due to the complex relationships between operational activities, economic developments, managerial incentives, and financial reporting rules. While early studies suggested that earnings changes follow a random walk and are unpredictable, recent research has focused on developing models that can predict the direction of future earnings changes.
The authors review various approaches to earnings prediction, including the use of financial statement variables, financial ratios, mixed data sampling regression, and ensemble algorithms. They also mention studies that have explored the predictive power of text data, such as annual report readability and the tone of financial reports and press releases. However, these studies have mostly relied on within-sample analyses and have not examined the out-of-sample predictive power of text data for future earnings.
To address this research gap, the paper focuses on the predictive power of vocal cues derived from manager speech during earnings conference calls. Earnings conference calls are deemed suitable for the study because they are common, significant information events attended by analysts and investors. The questions asked during the Q&A session of these calls can elicit affective responses and reveal managers’ emotional states, which may be informative about future business outcomes.
The authors highlight the importance of vocal cues as a channel of emotional communication and suggests that they provide a different perspective on an individual’s emotional state compared to the content of verbal speech. Vocal cues are argued to be less controlled and can unintentionally reveal emotional states. The evaluation of situational stimuli, such as questions during earnings conference calls, is believed to influence managers’ emotional states and implicit valuation of firm performance.
Prior research in psychology, neuroscience, accounting, and finance has provided evidence that vocal cues convey useful information about future firm performance and financial misreporting. However, there have been limited studies examining the out-of-sample predictive power of vocal cues, partly due to the challenges of manually processing audio recordings. This study aims to overcome this limitation by using machine learning algorithms and forced alignment techniques to automatically detect and analyze CEO speech in earnings conference calls.

2. Data
The data collection process and the approach to predicting the direction of earnings changes in US public firms are described. The researchers collected financial information from Refinitiv Eikon and obtained audio recordings and text transcripts from earnings conference calls from 2015 to 2020. They used reinforced alignment algorithms to synchronize the audio recordings with the corresponding transcripts and identify CEO speech during the Q&A session. After aligning and validating the audio segments, they merged them into a single audio file for each conference call. They ended up with a final sample of 8,436 firm-years from 2,372 unique firms.
The researchers focused on predicting the direction of one-year-ahead earnings changes because it is a more reliable indicator than predicting the magnitude of changes. They estimated detrended earnings per share (EPS) changes to account for firm-specific trends and coded earnings increases as ones and earnings decreases as zeros for binary classification. They used a rolling-forward splitting scheme to train and test the prediction models, gradually shifting the training set forward in time while maintaining a constant number of training years. The models were evaluated using the area under the receiver operating characteristic curve (AUC), which measures the model’s ability to rank predictions, as well as accuracy, precision, and negative predictive value. Overall, the study used a large sample and aimed to predict the direction of earnings changes in US public firms based on CEO speech during earnings conference calls.

3. Models
This section of the article discusses the implementation challenges and models used for analyzing vocal cues in a corporate context. The authors highlight two important characteristics of the human voice that need to be considered: the granularity of speech and the sequential nature of speech.
To address these characteristics, the authors propose three vocal cue models. The first model, Vocal Cues I, uses a Convolutional Neural Network (CNN) that processes spectrograms of managers’ voices. Spectrograms capture the frequency and amplitude variations of sound over time and allow for granular and sequential analysis of vocal cues. The authors use an ensemble approach to combine probability estimates from multiple spectrograms to make predictions. More details in section 4.1.2.

The second model, Vocal Cues II, utilizes wav2vec 2.0, a deep learning architecture designed for automatic speech recognition. This model processes raw waveforms and leverages a multilayer CNN, a quantizer, and a transformer to obtain contextualized representations of speech units. The authors fine-tune the pre-trained wav2vec 2.0 model for the classification of future earnings changes. More details in section 4.1.3.

The third model, Vocal Cues III, employs a Support Vector Machine (SVM) with phonetic features. This model considers phonetic characteristics of CEO speech, but it does not account for deep and sequential analysis of vocal cues. More details in section 4.1.4.
In terms of benchmark models, the authors use two models proposed by Chen et al. (2022) for earnings prediction based on financial data. The first benchmark model is a Random Forest that utilizes high-dimensional financial variables. The second benchmark model is a Stochastic Gradient Boosting algorithm that grows trees based on the previous tree’s forecast error.
Overall, the authors emphasize the computational demands and memory constraints associated with granular and sequential analyses of vocal cues. They also acknowledge the potential limitation of analyzing only 60 seconds of CEO speech but justify it based on existing research and practice.

Some results:
The results presented in Table 3 show that the estimated out-of-sample excess returns generated from trading strategies based on managers’ vocal cues are both statistically and economically significant. The monthly portfolio returns range from 9.34% to 17.03% for the CNN model, 5.43% to 5.61% for wav2vec 2.0, and 7.22% to 8.12% for the SVM model. On average (median), the annualized out-of-sample excess returns for all trading strategies utilizing vocal cues are 8.79% (7.67%). These findings are particularly meaningful because the portfolios have low turnover rates, meaning they are formed based on predicted one-year-ahead earnings changes and remain unchanged throughout the year.
These results suggest that the models based on vocal cues have economic relevance for investors. Although the AUC performance of the models already indicated this, it was possible that investors were already incorporating vocal cue information into their trading decisions. If that were the case, even though the models had significant AUCs, they would not have been able to generate significant excess returns because the information would have already been reflected in stock prices. However, based on the observed excess returns, it can be concluded that investors are not currently considering the information signals in vocal cues when making their investment decisions.

Hedge for stocks during bear markets
Not known - Probably not.