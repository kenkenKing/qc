This paper explores how investors navigate large volumes of financial information in corporate reports like 10-K and 10-Q, which have grown significantly in length and complexity. Traditional text analysis methods, such as dictionary-based sentiment analysis, struggle to identify key insights due to this complexity and companies’ adaptive language, leading to underperforming sentiment-based investment strategies. As a solution, the paper introduces FtBERT, a fine-tuned version of BERT specifically trained on financial data, which outperforms other methods in predicting future earnings surprises. The study demonstrates that while traditional sentiment analysis and bag-of-words approaches offer limited effectiveness, FtBERT’s specialized training allows it to excel at capturing financial information. The paper also reveals that longer corporate reports can obscure critical insights, causing confusion for investors. This finding underscores the need for more sophisticated NLP techniques in financial analysis, suggesting that future research should focus on tailoring these methods to the unique challenges of financial texts.

Fundamental reason
When processing large amounts of complex financial information, economic agents often struggle with distinguishing relevant insights from overwhelming details. Traditional text analysis methods, like dictionary-based sentiment analysis, can miss subtle signals due to companies’ deliberate use of language to obscure or emphasize information. As corporate filings such as 10-K and 10-Q reports grow longer and more complex, these traditional methods become less effective at discerning accurate insights about a company’s future financial performance, leading to investor inattention and potential misinterpretation of data. Given this, there’s a need for advanced Natural Language Processing (NLP) models that can process and extract meaningful information from financial texts, despite their complexity. Models like BERT and its fine-tuned version, FtBERT, offer a solution by providing a deeper understanding of textual context and predicting future earnings surprises with greater accuracy. By capturing the subtle relationships between words, sentences, and paragraphs, FtBERT outperforms traditional sentiment analysis methods, suggesting that more sophisticated NLP techniques tailored to financial contexts are critical for helping economic agents navigate the vast amounts of data they face.

Simple trading strategy
This paper describes a comprehensive data analysis process using financial filings from the SEC’s EDGAR database, alongside stock return data from CRSP and earnings forecasts from I/B/E/S. The primary aim is to analyze corporate disclosures, focusing on the Management Discussion and Analysis (MD&A) and Risk Factors (RF) sections in 10-K, 10-K405, 10-KSB, and 10-Q filings from 1993 to 2021. The detailed analysis framework involves several key components:

Data Acquisition and Parsing
SEC Filings: The initial data source is the SEC’s EDGAR website, from which all relevant filings are retrieved for the analysis period (1993–2021). Parsing these filings involves cleaning and normalizing the text, removing non-essential elements like markup tags, tables, and ASCII graphics. Filings are limited to one per firm per quarter, using 10-Qs for the first three quarters and 10-Ks for the final quarter of each fiscal year.
Targeted Sections: The primary focus is on the MD&A and RF sections. The MD&A section contains management’s commentary on financial statements, industry trends, and future outlook, while the RF section outlines potential risks that could affect business operations or stock prices.
Regulations and Compliance: Companies are legally obliged to disclose significant risk factors according to Regulation SK (Item 305(c), SEC 2005). The analysis accounts for the variability in subsection titles across filings and ensures that at least 250 words are present in the MD&A section to avoid cases where information is deferred to shareholders’ annual reports.
Stock Data and Earnings Surprise
Stock Returns: Monthly stock returns data is obtained from CRSP, focusing on common shares listed on the NYSE, Amex, or NASDAQ. Adjustments are made for delisting returns, and low-priced firms with stock prices smaller than $5 are excluded.
Earnings Surprise Calculation: The Institutional Brokers Estimate System (I/B/E/S) provides data for analysts’ earnings forecasts and actual earnings, allowing the calculation of Standardized Earnings Surprises (SUE) to measure the difference between actual earnings per share and analysts’ forecasts, normalized by share price 20 days prior to the earnings announcement.
Linking Identifiers and Data Processing
Linking Identifiers: The SEC’s Central Index Key (CIK) is matched with the CRSP stock identifier (permno), and the IBES identifier is linked with permno through the IBES-CRSP linking table.
Natural Language Processing (NLP) and Sentiment Analysis: The document outlines several NLP methodologies, such as converting text to lowercase, expanding contractions, removing stop words, and lemmatization. A variety of sentiment analysis approaches are applied, including the Loughran and McDonald finance context dictionary to measure sentiment and FinBERT to classify the sentiment of individual sentences.
Machine Learning Models for Predictive Analysis
Baseline Machine Learning Models: Linear and nonlinear models like OLS, Lasso, Elastic Net, Random Forest, XGBoost, and Support Vector Regression are used to process the high-dimensional textual data. Feedforward neural networks are also employed as a nonlinear benchmark.
Large Language Models (LLMs) and Transformer Approach: Advanced models like BERT, RoBERTa, and GPT-3 are discussed, highlighting the need to handle long input sequences by splitting them into smaller chunks. The document proposes a hierarchical Transformer architecture to process large chunks of text recursively, providing a global understanding of the MD&A and RF sections. This approach combines pre-trained BERT encoders, a Transformer layer with attention mechanisms, and a linear predictor to output the predicted normalized rank.
Training and Validation Approach
The analysis uses a sequential training and validation approach, expanding the training window each year to simulate out-of-sample predictions. This allows for continuous model evaluation and refinement, ensuring robust predictions based on corporate disclosures and financial data.

Overall, this document presents a comprehensive framework for extracting and analyzing financial data, with a focus on corporate disclosures and advanced NLP methodologies for sentiment analysis and predictive modeling.

The empirical analysis focuses on comparing the performance of various models in predicting financial data, specifically stock returns and earnings surprises. Models are grouped into three categories for evaluation:

Lexicon-based sentiment analysis: This group includes models that identify sentiment based on keywords, such as LM (Loughran-McDonald) negative sentiment, FinBERT (a BERT model trained on financial data), and the length of certain sections in reports (like Management Discussion & Analysis – MD&A).
Bag-of-words models with different regression approaches: This group uses various regression methods, such as Ordinary Least Squares (OLS), Elastic Net (EN), Lasso, and Support Vector Regression (SVR), to evaluate the predictive power of sentiment and text-based metrics on financial data.
Large Language Models (LLMs) trained for financial applications: These are advanced machine-learning models like FrozenBERT and FtBERT that are specifically trained to understand financial text and objectives.
Model Comparison and Portfolio Performance
To evaluate these models, the analysis considers their ability to predict stock returns and portfolio performance over time. Traditional metrics such as Out-of-Sample R² (OOS R²) and Mean Squared Error (MSE) are compared with more dynamic portfolio-based approaches. The rationale is that in financial data, especially with high variance, traditional metrics might not fully capture the predictive performance.

An innovative portfolio management strategy ranks stocks by predicted earnings surprises and then creates quintile portfolios, buying the top quintile and selling the bottom quintile. This “High-minus-Low” (H–L) strategy is used to test models’ predictive accuracy, as stock returns typically follow unexpected earnings surprises.

Key Findings
Group 1 (Lexicon-based models): Generally, these models showed mixed results in predicting future stock performance. For instance, the MD&A section length, a simple measure, was found to outperform more complex sentiment-based measures in identifying positive information. Conversely, FinBERT, a sentiment-based classifier, showed unexpected results with high negativity scores yielding higher returns.
Group 2 (Bag-of-words models with regression): Most models in this group failed to deliver consistent performance in predicting stock returns and earnings surprises. Some, like XGBoost and Random Forest, generated significant returns for equally weighted portfolios but became insignificant when adjusting for risk or considering value-weighted portfolios.
Group 3 (LLMs): These models were the most consistent performers. FtBERT showed significant and consistent results in identifying both positive and negative information about future performance, with significant risk-adjusted returns in both equally weighted and value-weighted portfolios. FrozenBERT, though less consistent, still demonstrated a strong ability to predict financial outcomes, albeit with lower performance compared to FtBERT.
Robustness Tests and Regression Analysis
The analysis also includes additional robustness tests, such as cross-sectional and time-series regressions with time and firm fixed effects, and event study panel regressions to predict earnings surprises and cumulative abnormal returns following announcements. The results indicate that while FtBERT can predict future earnings surprises, its ability to predict price discovery around earnings announcements was superior to FrozenBERT.

Conclusion
Overall, Group 3 (LLMs) emerged as the clear winner in terms of predictive accuracy for stock returns and earnings surprises. FtBERT was particularly successful, outperforming other models and demonstrating consistent results across various robustness tests. Traditional models, especially those relying on simpler sentiment analysis or regression approaches, showed limitations in identifying reliable predictors for stock returns and earnings surprises.

Hedge for stocks during bear markets
Not known -